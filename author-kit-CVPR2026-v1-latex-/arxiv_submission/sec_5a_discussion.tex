\section{Discussion}
\label{discussion}

\paragraph{On the Practical Relevance of Domain Filtering.} One might assume that single-domain applications rarely encounter out-of-domain samples, making domain feature collapse a purely theoretical concern. We challenge this assumption on two fronts.

\textbf{First, out-of-domain samples bypass safeguards in real deployments.} Production ML systems regularly encounter distribution shifts due to data pipeline errors, sensor changes, or evolving deployment contexts. Examples include: pathology classifiers receiving chest X-rays due to mislabeled data, autonomous driving models encountering construction zones, satellite imagery systems experiencing sensor degradation, or agricultural monitors receiving multispectral images from upgraded sensors.

\textbf{Second, domain feature collapse affects nearby domains, not just distant ones.} While simple methods might filter obviously different domains, our results show that state-of-the-art OOD methods fail even on \emph{nearby} domains. As shown in Table \ref{tab:fprcolon}, ReAct achieves 41.0\% FPR@95 when distinguishing colon pathology (ID) from chest X-rays (OOD) -- both medical images. This demonstrates that domain feature collapse occurs for nearby domains that are far more likely to ``bleed into'' single-domain tasks. Preserving $I(\rvx_\rvd; \rvz) > 0$ through pretrained representations reduces this to 0.4\% FPR@95, showing that domain feature collapse is a real, practical problem requiring solutions beyond simple heuristics.

\paragraph{Wide Domains.} On some datasets, DinoV2 Domain Filtering has difficulty with outliers, resulting in a very large distance threshold $t_\rvd$ and poor domain filter performance. The Rock dataset \citep{rock_data} would often set $t_\rvd \approx 1.78$, compared with the Colon dataset at $t_\rvd \approx 0.47$ and the Food dataset at $t_\rvd \approx 1.08$. By changing $p=0.99\to0.98$, we can reduce $FPR@95=52.5\to27.9 $ for the Rock dataset on out-of-domain OOD detection. One example of an outlier in the Rock dataset is an image of a marble countertop, as shown in Figure \ref{fig:rock} in the Appendix. See Appendix \ref{percentile} for a more detailed analysis of percentiles.

\paragraph{Performance Cap.} One major problem with domain filtering is the strict nature of its false positive rate. For in-domain data that is of a similar distribution to the training data, we expect a minimum false positive rate equal to $FPR = 1 - p$. We find that increasing $p$ works well if the domain is narrow, but can significantly harm out-of-domain performance if there are outliers; see Appendix \ref{percentile}.

\paragraph{Unseen Domains.} Readers may question the viability of domain filtering when both the ID set and OOD set are unknown to the domain filtering model. In other words, since a pretrained DinoV2 model has seen such a wide variety of images, it may have already seen images similar to those in the out of domain OOD set. To address this concern, we included the chest Xray dataset \citep{yang2023medmnist} to show that a pretrained DinoV2 can filter between two unseen medical domains quite well (achieving 0.1 FPR@95 with the colon dataset as ID and Chest Xrays as OOD). This demonstrates that the preservation of $I(\rvx_\rvd; \rvz) > 0$ in pretrained representations is not merely due to memorization, but reflects genuine domain-level feature retention.

\paragraph{Mutual Information Measurement.} It is important to clarify that our experiments do not directly measure or prove that $I(\rvx_\rvd; \rvz) = 0$ for supervised representations. Instead, they establish a lower bound on $I(\rvx_\rvd; \rvz)$ through OOD detection performance. By Fano's inequality, poor separation performance necessarily implies a low lower bound on mutual information, while high separation performance implies a high lower bound. We acknowledge that cases may exist where $I(\rvx_\rvd; \rvz)$ is large yet OOD detection performance remains poor, potentially due to inefficient utilization of domain information for detection. However, our results clearly establish that the lower bound on $I(\rvx_\rvd; \rvz)$ is substantially higher when using the two-stage detector with pretrained representations compared to supervised representations alone. This quantifies the information-theoretic gap predicted by our theory and demonstrates the practical benefit of preserving domain information in the representation space.

