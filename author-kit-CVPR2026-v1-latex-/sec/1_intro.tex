\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{domain_feature_collapse.jpg}
  \caption{Domain Feature Collapse: Supervised learning on single-domain data inevitably produces representations where domain information is lost ($I(\rvx_\rvd; \rvz) = 0$). This leads to catastrophic failure in OOD detection since models cannot distinguish in-domain from out-of-domain samples without domain-specific features.}
  \label{fig:domain_collapse}
  \vspace{-0.5cm}
\end{figure}

The deployment of deep neural networks (DNNs) in safety-critical domains -- such as autonomous driving \citep{ramanagopal2018failing}, biometric authentication \citep{wang2021deep}, and medical diagnostics \citep{bakator2018deep} -- necessitates robust out-of-distribution (OOD) detection to flag inputs whose labels were not seen during training. While state-of-the-art OOD detection methods demonstrate strong performance on established benchmarks \citep{zhang2023openood}, these benchmarks almost exclusively use multi-domain in-distribution (ID) sets such as CIFAR10/100 \citep{cifar10} and ImageNet \citep{deng2009imagenet}. This creates a critical gap: current methods are tailored to scenarios where ID data is inherently diverse across domains, yet many real-world applications -- medical imaging \citep{zhang2021out}, satellite imagery \citep{ekim2024distribution}, and agriculture \citep{saadati2024out} -- deploy models on single, narrow domains with highly consistent data characteristics.

We identify a critical failure mode unique to single-domain settings: \textbf{domain feature collapse}. We provide the first formal proof that supervised learning on single-domain data inevitably produces representations with $I(\rvx_\rvd; \rvz) = 0$, where $\rvx_\rvd$ represents domain features and $\rvz$ the learned representation. This counterintuitive result -- better class-specific optimization leads to worse domain robustness -- is a mathematically inevitable consequence of information bottleneck optimization. Models learn to rely solely on class-specific features while discarding domain-specific features (e.g., knowing an image is an X-ray helps OOD detection but not disease classification). Critically, this failure mode rarely occurs for multi-domain ID sets, making it difficult to identify in commonly used benchmarks.

We introduce Domain Bench, a benchmark covering diverse single-domain datasets (medical imaging, agriculture, satellite imagery), and propose domain filtering, a two-stage solution that addresses the root cause by preserving domain information in the representation space.

Our key contributions are:
\begin{itemize}
\item \textbf{Theoretical}: We prove that single-domain supervised learning inevitably leads to $I(\rvx_\rvd; \rvz) = 0$ under information bottleneck optimization, revealing a fundamental trade-off where optimal class prediction necessitates domain information loss. We extend this using Fano's inequality to quantify partial collapse in practical scenarios.

\item \textbf{Architectural Solution}: We propose domain filtering, a method-agnostic framework that addresses the $I(\rvx_\rvd; \rvz) = 0$ problem by using representation spaces (pretrained models) that preserve domain information. This works with diverse base detectors (MSP, Energy, Mahalanobis, KNN, ReAct, Scale, NCI).

\item \textbf{Benchmark}: We introduce Domain Bench with multiple single-domain datasets (medical imaging, agriculture, satellite imagery) to evaluate OOD detection in this under-explored setting\footnote{see supplementary material for anonymized repository}.
\end{itemize}
