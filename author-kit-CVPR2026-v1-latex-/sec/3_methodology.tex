\section{Methodology}
\label{sec:method}

\subsection{Dataset Domain and Domain Features}

We define the dataset's domain $\rvd$ via a domain labeling function $f_{\rvd}(\rvx)$. In single-domain settings, all training data shares the same domain: $\forall \rvx \in  \{\rvx : f_{\rvy}(\rvx) \in  \sY_{in}\}, f_{\rvd}(\rvx) = \rvd_1$, implying $\forall \rvx \in \{\rvx : f_{\rvd}(\rvx) \neq \rvd_1\}, f_{\rvy}(\rvx) \notin \sY_{in}$.

\begin{definition}
\emph{Domain Features}. Given a dataset with domain $\rvd$ determined by $f_{\rvd}(\rvx)$, we define domain features $\rvx_\rvd$ as the minimal subset of features of $\rvx$ sufficient for $f_{\rvd}$, independent of the minimal sufficient class features $\rvx_\rvy$: $I(\rvx_\rvd ; \rvx_\rvy) = 0$.
\label{definedomainfeatures}
\end{definition}

Examples include medical X-rays \citep{yang2023medmnist}, geology \citep{rock_data}, or satellite imagery \citep{helber2019eurosat}. Domains exist in a hierarchy (e.g., cats $\subset$ mammals $\subset$ animals), with wider domains having fewer domain features. Multi-domain datasets like ImageNet have $|\{\rvx_\rvd\}| \approx 0$ as their class diversity spans multiple domains.

\subsection{Domain Feature Collapse}

We provide the first formal proof that domain feature loss is an \emph{inevitable consequence} of information bottleneck optimization under single-domain training. Any supervised model will learn representations with $I(\rvx_\rvd, \rvz) = 0$ under full bottleneck compression -- a counterintuitive result where better class-specific optimization necessarily leads to worse domain robustness.
\begin{theorem} \textcolor{black}{Strict} Domain Feature Collapse in the Minimal Sufficient Statistic. \\
    Let $\rvx$ come from a distribution. $\rvx$ is composed of two independent variables $\rvx_\rvd$ and $\rvx_\rvy$, where $\rvx_\rvd$ is a set of domain features as per definition \ref{definedomainfeatures}. Let $\rvd$ be a domain label random variable generated from the labeling function $f_\rvd(\rvx_\rvd)$. In the single-domain training setting, $f_\rvd(\rvx_\rvd) = \rvd_1$ for all $\rvx$ in the training set, where $\rvd_1$ is a constant domain value. Let $\rvy$ be a class label generated from $f_\rvy(\rvx_\rvd, \rvx_\rvy) = \rvy$. Let $\rvz$ be any sufficient representation of $\rvx$ for $\rvy$ that satisfies the sufficiency definition \ref{definesuff} and minimizes the loss function $\mathcal{L} = I(\rvx_\rvd \rvx_\rvy; \rvz) - \beta I(\rvz;\rvy)$. The possible $\rvz$ that minimizes $\mathcal{L}$ and is sufficient must meet the condition $I(\rvx_\rvd; \rvz) = 0$.



    \label{mainbodygenloss}
\end{theorem}

\begin{remark}
\textbf{Significance.} Theorem \ref{mainbodygenloss} is non-trivial, requiring: (1) decomposing mutual information under independence constraints, (2) analyzing the optimization landscape, (3) establishing sufficiency conditions, and (4) proving all alternatives violate sufficiency or have higher loss. Critically, the optimization process itself -- not architecture or hyperparameters -- guarantees domain feature loss, explaining why state-of-the-art methods struggle with out-of-domain detection.
\end{remark}

Detailed proof is in Appendix \ref{genloss}. Intuitively, the minimal sufficient representation cannot encode information independent of the learning objective. Since $\rvx_\rvd$ is independent of class features, compression discards domain features. The model learns $\hat{\rvy} = g(\rvx_\rvy)$ instead of the desired $\hat{\rvy} = g(\rvx_\rvd, \rvx_\rvy)$ because $\rvx_\rvd$ is not predictive of class in the training data. For example, a model might classify "Barney" (a purple cartoon character) as a dinosaur, ignoring that it is purple.

While full compression may not occur in practice \citep{tishby2015deep}, Fano's Inequality (Appendix \ref{fano}) extends our theory to partial compression, showing that even small $I(\rvx_\rvd; \rvz)$ leads to unreliable OOD detection. Fano's inequality also allows interpreting our experimental results as establishing a lower bound on $I(\rvx_\rvd; \rvz)$: high separation performance (low FPR@95) implies high $I(\rvx_\rvd; \rvz)$, while poor performance implies low $I(\rvx_\rvd; \rvz)$. This quantifies information loss in supervised representations.

\subsection{Limitations of Current Solutions}

\paragraph{Fine Tuning.} Methods like Energy \citep{liu2020energy} and MOS \cite{huang2021mos} fine-tune pretrained models on ID data, but may not prevent domain feature collapse due to catastrophic forgetting \cite{mccloskey1989catastrophic}.

\paragraph{Pretrained Models.} Zero-shot methods using CLIP \cite{radford2021learning,esmaeilpour2022zero} struggle with narrow domains. \cite{yangcan} shows pretrained models fail on the adjacent OOD benchmark (in-domain but OOD samples, e.g., new disease types) due to lacking relevant class features when pretraining and ID domains differ significantly.

\paragraph{Unsupervised OOD Detection.} Methods using autoencoders \cite{zhou2022rethinking}, contrastive learning \cite{sehwag2021ssd}, or diffusion models \cite{liu2023unsupervised} may preserve $I(\rvx_\rvd;\rvz) > 0$ but require domain-specific adaptation and also struggle with adjacent OOD \citep{yangcan} due to lacking class features.

\paragraph{Auxiliary Loss Functions.} Auxiliary losses may align with domain features but are domain-specific and do not generalize (features for domain A may be irrelevant for domain B).

\subsection{Domain Filtering: A Solution}
\label{sec:domain_filtering}

\paragraph{Design Rationale.} Our theorem reveals supervised training on single-domain data inevitably produces $I(\rvx_\rvd; \rvz) = 0$. The solution lies in the \emph{representation space} itself: using pretrained models (trained on diverse multi-domain data) to preserve domain information that supervised training discards. This is an architectural insight -- we use \emph{distinct representation spaces} for different detection tasks: pretrained features for domain filtering (preserving $I(\rvx_\rvd; \rvz) > 0$) and supervised features for class-based OOD detection (maximizing $I(\rvx_\rvy; \rvz)$), combining their complementary strengths.

\subsubsection{Two Stage Detector: Domain Filtering + OOD Detector}

We use a two-stage process: (1) a pretrained network determines if a sample is in-domain, (2) an OOD detector determines if in-domain samples are in-distribution. This assumes no in-distribution sample is out-of-domain, consistent with our definitions. \textbf{Crucially, each stage operates in a distinct representation space}: the first stage uses pretrained features (preserving $I(\rvx_\rvd; \rvz) > 0$) while the second stage uses supervised features (optimized for class discrimination). This is not simply running the same detector twice, but rather leveraging complementary representation spaces.

\paragraph{Implementation.} We use a KNN-based domain filter (similar to \cite{sun2022out}), but the novelty is the \emph{framework}: domain filtering must operate in a representation space preserving $I(\rvx_\rvd; \rvz) > 0$, which supervised representations cannot provide. Specifically, the domain filter computes KNN distances in the \emph{pretrained DinoV2 feature space}, while the second-stage OOD detector operates in the \emph{supervised model's feature space}. We calibrate the threshold $\rvt_\rvd$ such that $P(f_{knn}(\{\rvx \in \sX_{train}\}) \leq \rvt_\rvd) = p$ with $p = 0.99$ and $K = 50$, flagging samples with $f_{knn} > \rvt_d$ as OOD. See Algorithm \ref{twostage} in Appendix \ref{twostageapp}. We also test $p \in \{0.98, 0.999\}$ following OpenOOD's hyperparameter tuning. We demonstrate domain filtering with three second-stage detectors (Mahalanobis, KNN, ReAct), though the framework theoretically works with any OOD detector.

\subsubsection{Adjacent, Near, and Far OOD Benchmarks}
\label{adjacentnearfar}

Standard benchmarks distinguish near OOD (semantically different but visually similar) from far OOD (both semantically and visually dissimilar) \citep{fort2021exploring,yang2022openood,zhang2023openood}. For single-domain ID data (e.g., X-rays), both near and far OOD are out-of-domain, and the domain filter detects them effectively.

In contrast, the adjacent OOD benchmark \citep{yangcan} tests \emph{in-domain} OOD samples by training on a subset of classes (e.g., 2/3) and evaluating on held-out classes (1/3) from the same dataset. These held-out classes share domain features $\rvx_\rvd$ (e.g., satellite imagery classes "Forest" and "River" share domain features with training classes "Residential" and "Industrial"). This isolates in-domain OOD samples, which are safety-critical as unknown classes from the same domain may appear in deployment. The domain filter alone performs poorly on adjacent OOD as samples are correctly identified as in-domain, necessitating the second-stage detector.

\subsubsection{Ensembling vs Filtering}

Ensemble methods have been used in uncertainty estimation and OOD detection before, such as \citep{lakshminarayanan2017simple} and \citep{pmlr-v235-xu24ae}. However, any ensemble would have to contend with a large performance gap between the two models. If we assume that the secondary model is good at out-of-domain OOD, its score would be dragged down by the primary model, which would be worse at out-of-domain OOD. Similarly, the primary model would be dragged down on in-domain OOD by the secondary model. 

Domain filtering significantly reduces the negative impacts of ensembling by allowing the correct model to dominate the OOD score based on the domain of the sample. This allows us to maintain good in-domain OOD detection performance by limiting our negative impact on the primary model. 

