\section{Experimental Results}
\label{sec:experiments}

\subsection{Experimental Setup}

For each narrow domain dataset, we generate a ID train, ID validation, ID test, and OOD test dataset using a unique seed. After training with ID data on one of the three methods below, we evaluate multiple OOD detection algorithms using the weights with the highest validation classification accuracy. For each OOD detection algorithm, we use the default postprocessor provided by OpenOOD \citep{zhang2023openood}. We also implement two additional two-stage post processors, combining a pretrained DinoV2 ViTs14 \citep{oquab2023dinov2} with ReAct \citep{sun2021react} or KNN \citep{sun2022out}; more information can be found in Appendix \ref{setup}. Code for running experiments and benchmarks is included in supplementary material.
\begin{itemize}
\item \textbf{Cross Entropy Resnet50 (CE Resnet).} We fine tune a pretrained Resnet50 for $300$ epochs using an SGD optimizer with an initial learning rate of $0.1$. 
\item \textbf{Cross Entropy DinoV2 (CE DinoV2).} We fine tune a pretrained DinoV2 ViTs14 for $75$ epochs using an Adam optimizer with an initial learning rate of $0.0001$.
\item \textbf{Supervised Contrastive Learning Resnet50 (SC Resnet).} We train a Resnet50 using supervised contrastive learning for $500$ epochs using an SGD optimizer with an initial learning rate of $0.5$ and a temperature of $0.5$. 
\end{itemize}







\subsection{In and Out-of-Domain OOD Benchmarking}
\label{domainbench}

Our experimental evaluation distinguishes between two types of OOD samples based on domain characteristics:

\paragraph{Out-of-Domain OOD.} These are samples from entirely different domains than the training data. For example, when training on EuroSat (satellite imagery), out-of-domain OOD samples include MNIST (handwritten digits), SVHN (street numbers), medical images, etc. These samples differ fundamentally in their domain features $\rvx_\rvd$ from the training domain. For the out-of-domain OOD benchmark, we use the following datasets as provided by OpenOOD \cite{zhang2023openood}: MNIST \citep{lecun1998gradient}, SVHN \citep{netzer2011reading}, Texture \citep{cimpoi2014describing}, Places365 \citep{zhou2017places}, Cifar10/100 \citep{cifar10}, and Tiny Image Net \citep{deng2009imagenet}. We also add samples from Chest X-rays \citep{yang2023medmnist} into the out-of-domain OOD benchmark, as it does not share a domain with the Tissue or Colon ID datasets.

\paragraph{In-Domain OOD.} These are samples that share the same domain as the training data but belong to classes not seen during training. For example, when training on a subset of EuroSat classes (e.g., "Residential" and "Industrial"), in-domain OOD samples are other EuroSat classes (e.g., "Forest" and "River") that were held out. These samples share domain features $\rvx_\rvd$ (satellite imagery characteristics) but have different class labels. To construct in-domain OOD sets, we use the adjacent OOD benchmark methodology proposed by \cite{yangcan}, which randomly selects $1/3$ of the original dataset's classes to serve as in-domain OOD. Because this class selection affects performance variance, we repeat our experiments 5 times with 5 different random seeds.

\subsection{Variance Across Seeds}

A consequence of repeating the adjacent OOD benchmark across multiple seeds is a significant variance in performance across seeds, due to selecting different classes as OOD. However, we observe that adding the domain filter consistently improves out-of-domain OOD performance while having very little impact on in-domain performance. We provide additional analysis on the statistical significance in Appendix \ref{variance}.

\subsection{Single-Domain Datasets}

We use $11$ single-domain datasets where all training samples share common domain features $\rvx_\rvd$, ensuring domain features are independent of class features $\rvx_\rvy$ and subject to compression as predicted by Theorem \ref{mainbodygenloss}. Datasets include: \textbf{Butterfly} (species classification \citep{AIPlanet_DataSprint107_2024}), \textbf{Cards} (playing cards \citep{card_data}), \textbf{Colon} (pathology \citep{yang2023medmnist}), \textbf{Eurosat} (satellite land use \citep{helber2019eurosat}), \textbf{Fashion} (FashionMNIST \citep{fashion}), \textbf{Food} (Food101 \citep{food}), \textbf{Garbage} (waste materials \citep{single2023realwaste}), \textbf{Plant} (leaf diseases \citep{plant}), \textbf{Rock} (minerals \citep{rock_data}), \textbf{Tissue} (kidney cortex \citep{yang2023medmnist}), and \textbf{Yoga} (poses \citep{yoga_data}). See Appendix \ref{dataset} for details and sample images.

\subsection{Results}

We report highlighted results in Tables \ref{tab:summary}, \ref{tab:fpr95}, and \ref{tab:fprcolon}. These tables are a representative sample of the detailed results, see Appendix \ref{expresult}. All methods have some level of difficulty in out-of-domain OOD detection, even though datasets like MNIST should not be challenging. On some in-distribution datasets, such as EuroSat, we observe that many methods obtain similar or better in-domain OOD detection performance compared to out-of-domain OOD detection performance. These results confirm the theoretical findings regarding domain feature collapse from Theorem \ref{mainbodygenloss}.

Crucially, our experimental results establish a lower bound on $I(\rvx_\rvd; \rvz)$ for each representation. The poor out-of-domain detection performance of supervised models (e.g., MSP with FPR@95 of $53.0$\% on Colon) indicates a low lower bound on $I(\rvx_\rvd; \rvz)$ in supervised representations. By Fano's inequality, if $I(\rvx_\rvd; \rvz)$ were even lower, performance would necessarily degrade further. In contrast, the pretrained DinoV2 domain filter achieves FPR@95 of $0.0$\% on the same dataset, demonstrating a substantially higher lower bound on $I(\rvx_\rvd; \rvz)$. This quantifies the information-theoretic gap between supervised and pretrained representations predicted by our theory.

It is also important to note that there is high variance for individual out-of-domain datasets when benchmarking. For example, when models are trained on the Colon ID dataset, they tend to struggle with detecting MNIST patterns as OOD. We observe that certain OOD sets are more problematic depending on the ID set. This suggests that only some OOD datasets contain features that could be similar to the ID class features, supporting the idea that domain feature collapse occurs from an over-reliance on class features and the inability to identify domain features. 

In every case, adding a domain filter reduces the FPR@95 for out of domain OOD detection by a significant margin, sometimes trivializing the threat of out of domain OOD samples. For example, ReAct performs best in-domain on the Colon dataset, but suffers from extremely high FPR@95 of $61$\% on out of domain OOD samples. Adding a domain filter reduces this FPR rate to $0.7$\%, effectively eliminating the problem of out-of-domain OOD detection. In most cases, adding the domain filter reduces in domain performance by a marginal amount. 

These results demonstrate that domain feature collapse is a real problem across a wide variety of datasets. We also observe that domain filtering is a generally applicable  solution to address domain feature collapse. % but there are some trade offs in terms of... 

\begin{table*}
\centering
\caption{Summary OOD Performance Across All Datasets Reported As (In-Domain OOD Score)/(Out-of-Domain OOD Score). We exclude the Rock dataset from this summary as it is an outlier for reasons explained in Section \ref{discussion}.  Best scores are in bold (excluding PT KNN baseline). The domain filter methods are italicized. SC Resnet is not compatible with OOD methods that use logits.  See Appendix \ref{oodref} for OOD detection method descriptions and Appendix \ref{expresult} for more detailed results.}
\label{tab:summary}
\begin{tabular}{lllllll}
\toprule
 & \multicolumn{3}{l}{FPR@95 (Lower is Better)} & \multicolumn{3}{l}{AUROC (Higher is Better)} \\
 & CE DinoV2 & CE Resnet & SC Resnet & CE DinoV2 & CE Resnet & SC Resnet \\
Method &  &  &  &  &  &  \\
\midrule
PT KNN & 79.7 / 0.9 & 79.7 / 0.9  & 79.7 / 0.9  & 65.1 / 99.6 & 65.1 / 99.6 & 65.1 / 99.6 \\
MSP & 65.4 / 43.0 & 61.8 / 38.9 & NA & 75.1 / 82.0 & 78.3 / 87.4 & NA \\
Energy & 65.0 / 37.3 & 65.3 / 41.4 & NA & 75.3 / 85.6 & 78.0 / 87.6 & NA \\
Mahalanobis & 62.5 / 18.5 & 59.9 / 16.2 & 62.3 / 34.7 & 75.9 / 93.4 & 78.4 / 94.4 & \textbf{78.9} / 87.6 \\
NCI & 66.7 / 35.3 & 74.5 / 36.1 & NA & 74.1 / 86.6 & 73.3 / 88.5 & NA \\
KNN & \textbf{61.9} / 25.4 & 64.4 / 25.8 & \textbf{61.5} / 32.9 & 75.8 / 91.0 & 76.1 / 91.1 & 78.0 / 87.8 \\
ReAct & 64.2 / 36.4 & 71.9 / 47.7 & NA & 75.9 / 86.3 & 74.4 / 84.9 & NA \\
\textit{DF + KNN} & 65.2 / 3.2 & 64.3 / \textbf{2.5} & 63.8 / \textbf{3.2} & 73.9 / 99.0 & 75.8 / \textbf{99.2} & 76.2 / \textbf{99.0} \\
\textit{DF + ReAct} & 64.3 / 3.7 & 72.3 / 4.1 & NA & 75.7 / 98.8 & 73.8 / 99.1 & NA \\
\textit{DF + MDS} & 62.1 / \textbf{2.9} & \textbf{60.0} / 11.8 & 62.4 / 11.6 & \textbf{76.1} / \textbf{99.0} & \textbf{78.6} / 96.3 & 78.1 / 95.2 \\
\bottomrule
\end{tabular}
\end{table*}



\begin{table*}
\centering
\caption{Summary FPR@95 OOD Performance Across All Datasets for Selected ID Datasets Reported As (In-Domain OOD Score)/(Out-of-Domain OOD Score) for the CE Resnet model. Best scores are in bold (excluding PT KNN baseline). See Appendix \ref{oodref} for OOD detection method descriptions and Appendix \ref{expresult} for more detailed results.}
\label{tab:fpr95}
\begin{tabular}{lllllll}
\toprule
& Colon & Eurosat & Food & Garbage & Rock & Tissue \\
Method &  &  &  &  &  &  \\
\midrule
PT KNN & 67.4 / 0.0 & 69.1 / 0.3 & 80.0 / 0.6 & 87.2 / 0.4 & 91.9 / 6.6 & 89.3 / 0.0 \\
MSP & 59.1 / 53.0 & \textbf{41.3} / 49.8 & 74.9 / 63.7 & 68.0 / 42.0 & 85.8 / 71.8 & 84.2 / 76.6 \\
Energy & 61.0 / 70.7 & 42.5 / 50.1 & 75.2 / 62.9 & 78.7 / 54.3 & 86.7 / 71.2 & 84.4 / 79.2 \\
Mahalanobis & 40.8 / 12.5 & 51.4 / 13.7 & 76.8 / 52.1 & \textbf{59.8} / 13.9 & 83.1 / 44.2 & 91.4 / 3.8 \\
NCI & 74.5 / 24.8 & 72.7 / 57.1 & 80.4 / 65.4 & 74.2 / 31.4 & 75.9 / 64.0 & 84.5 / 35.7 \\
KNN & 40.0 / 13.2 & 48.4 / 31.3 & \textbf{73.3} / 62.7 & 77.9 / 33.3 & 77.3 / 61.8 & 92.6 / 31.6 \\
ReAct & \textbf{39.0} / 61.2 & 55.5 / 54.4 & 85.9 / 71.1 & 82.9 / 58.6 & 84.7 / 75.0 & \textbf{81.7} / 48.0 \\
\textit{DF + KNN} & 41.5 / \textbf{0.2} & 49.6 / \textbf{1.5} & 73.5 / \textbf{2.3} & 76.5 / 2.1 & 75.1 / 52.5 & 92.2 / 0.4 \\
\textit{DF + ReAct} & 40.6 / 0.7 & 65.2 / 4.3 & 86.4 / 2.2 & 82.9 / \textbf{1.8} & 84.9 / 61.0 & 81.9 / \textbf{0.7} \\
\textit{DF + MDS} & 40.4 / 6.9 & 51.4 / 10.4 & 76.6 / 39.1 & 61.6 / 12.7 & \textbf{82.0} / \textbf{39.8} & 91.3 / 0.9 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}
\centering
\caption{Detailed FPR@95 OOD Detection Performance for the Colon Dataset using the CE Resnet model. See Appendix \ref{oodref} for OOD detection method descriptions. Best scores are in bold (excluding PT KNN baseline).}
\label{tab:fprcolon}
\begin{tabular}{llllllllll}
\toprule
OOD Dataset & In Domain  & Chest & Cifar10 & Cifar100 & Mnist & Place365 & Svhn & Texture & Tin \\
Method &  (Adjacent) &  &  &  &  &  &  &  &  \\
\midrule
PT KNN & 67.4 & 0.1 & 0.0 & 0.0 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 \\
MSP & 59.1 & 3.3 & 46.8 & 66.9 & 38.2 & 36.9 & 63.5 & 96.1 & 72.4 \\
Energy & 61.0 & 3.1 & 89.1 & 92.1 & 41.1 & 75.5 & 77.4 & 98.5 & 89.3 \\
Mahalanobis & 40.8 & 16.7 & 8.7 & 9.0 & 27.7 & 6.1 & 15.6 & 13.2 & 3.1 \\
NCI & 74.5 & 11.0 & 24.2 & 24.5 & 14.4 & 29.2 & 42.4 & 28.5 & 24.4 \\
KNN & 40.0 & 12.4 & 11.9 & 11.6 & 26.7 & 4.3 & 16.0 & 17.8 & 5.0 \\
ReAct & \textbf{39.0} & 41.0 & 62.6 & 64.2 & 45.7 & 52.4 & 77.2 & 74.2 & 72.3 \\
DF + KNN & 41.5 & \textbf{0.2} & \textbf{0.2} & \textbf{0.2} & \textbf{0.3} & \textbf{0.1} & \textbf{0.2} & \textbf{0.3} & \textbf{0.1} \\
DF + ReAct & 40.6 & 0.4 & 0.8 & 0.8 & 0.5 & 0.6 & 0.9 & 0.9 & 0.8 \\
DF + MDS & 40.4 & 8.9 & 4.6 & 4.7 & 15.5 & 3.3 & 8.6 & 7.3 & 1.8 \\
\bottomrule
\end{tabular}
\end{table*}

