\section{Experimental Results}
\label{sec:experiments}

\subsection{Experimental Setup}

For each narrow domain dataset, we generate a ID train, ID validation, ID test, and OOD test dataset using a unique seed. After training with ID data on one of the three methods below, we evaluate multiple OOD detection algorithms using the weights with the highest validation classification accuracy. For each OOD detection algorithm, we use the default postprocessor provided by OpenOOD \citep{zhang2023openood}. We also implement two additional two-stage post processors, combining a pretrained DinoV2 ViTs14 \citep{oquab2023dinov2} with ReAct \citep{sun2021react} or KNN \citep{sun2022out}; more information can be found in Appendix \ref{setup}.
\begin{itemize}
\item \textbf{Cross Entropy Resnet50 (CE Resnet).} We fine tune a pretrained Resnet50 for $300$ epochs using an SGD optimizer with an initial learning rate of $0.1$. 
\item \textbf{Cross Entropy DinoV2 (CE DinoV2).} We fine tune a pretrained DinoV2 ViTs14 for $75$ epochs using an Adam optimizer with an initial learning rate of $0.0001$.
\item \textbf{Supervised Contrastive Learning Resnet50 (SC Resnet).} We train a Resnet50 using supervised contrastive learning for $500$ epochs using an SGD optimizer with an initial learning rate of $0.5$ and a temperature of $0.5$. 
\end{itemize}







\subsection{In and Out-of-Domain OOD Benchmarking}
\label{domainbench}
As noted in Section \ref{adjacentnearfar}, all OOD datasets can be reduced into in-domain and out-of-domain. For the out-of-domain OOD benchmark, we use the following datasets as provided by OpenOOD \cite{zhang2023openood}: MNIST \citep{lecun1998gradient}, SVHN \citep{netzer2011reading}, Texture \citep{cimpoi2014describing}, Places365 \citep{zhou2017places}, Cifar10/100 \citep{cifar10}, and Tiny Image Net \citep{deng2009imagenet}. We also add samples from Chest X-rays \citep{yang2023medmnist} into the out-of-domain OOD benchmark, as it does not share a domain with the Tissue or Colon ID datasets. 

To evaluate in domain OOD detection performance, we use the adjacent OOD benchmark proposed by \cite{yangcan}. Because the adjacent OOD benchmark samples a subset of ID classes to be considered as OOD, we repeat our experiments 5 times with 5 different random seeds.  

\subsection{Variance Across Seeds}

A consequence of repeating the adjacent OOD benchmark across multiple seeds is a significant variance in performance across seeds, due to selecting different classes as OOD. However, we observe that adding the domain filter consistently improves out-of-domain OOD performance while having very little impact on in-domain performance. We provide additional analysis on the statistical significance in Appendix \ref{variance}.

\subsection{Narrow Domain Datasets}

We use the following $11$ narrow domain datasets as the in-distribution dataset to evaluate the impact of domain feature collapse. These datasets often contain class features that are quite subtle and difficult for pretrained models to pick up. They are also considered single domain as per our earlier domain definition. Additional descriptions and sample images are provided in Appendix \ref{dataset}.

\textbf{Butterfly} – A butterfly species classification dataset \citep{AIPlanet_DataSprint107_2024}.

\textbf{Cards} – A playing card classification dataset by rank and suit \citep{card_data}.

\textbf{Colon} – A colon pathology dataset with different diseases labeled \citep{yang2023medmnist}.

\textbf{Eurosat} – A satellite images dataset for classifying different types of land use \citep{helber2019eurosat}.

\textbf{Fashion} – The FashionMNIST dataset describing different articles of clothing \citep{fashion}.

\textbf{Food} – The Food101 datasets \citep{food} with $101$ classes of different types of food.

\textbf{Garbage} – A dataset to classify the material of different waste objects \citep{single2023realwaste}.

\textbf{Plant} – A plant leaves dataset detailing different types of disease \citep{plant}.

\textbf{Rock} – A dataset of different types of rocks and minerals \citep{rock_data}.

\textbf{Tissue} – A kidney cortex microscope dataset with various types of tissue labeled \citep{yang2023medmnist}.

\textbf{Yoga} – A dataset of people performing different yoga poses from the internet \citep{yoga_data}.


% \textbf{Eurosat} - A satellite images dataset for classifying different types of land use \citep{helber2019eurosat}.

% \textbf{Yoga} - A dataset of people performing different yoga poses from the internet \citep{yoga_data}. 

% \textbf{Rock} - A dataset of different types of rocks and minerals \citep{rock_data}. 

% \textbf{Colon} - A colon pathology dataset with different diseases labeled \citep{yang2023medmnist}. 

% \textbf{Tissue} - A kidney cortex microscope dataset with various types of tissue labeled \citep{yang2023medmnist}.

% \textbf{Fashion} - The FashionMNIST dataset describing different articles of clothing \citep{fashion}.

% \textbf{Plant} - A plant leaves dataset detailing different types of disease \citep{plant}.

% \textbf{Food} - The Food101 datasets \citep{food} with 101 classes of different types of food. 

% \textbf{Butterfly} - A butterfly specifies classification dataset \citep{AIPlanet_DataSprint107_2024}.

% \textbf{Garbage} - A dataset to classify the material of different waste objects \citep{single2023realwaste}.

% \text{Cards} - A playing card classification dataset by rank and suit \citep{card_data}.

\subsection{Results}

We report highlighted results in Tables \ref{tab:summary}, \ref{tab:fpr95}, and \ref{tab:fprcolon}. These tables are a representative sample of the detailed results, see Appendix \ref{expresult}. All methods have some level of difficulty in out-of-domain OOD detection, even though datasets like MNIST should not be challenging. On some in-distribution datasets, such as EuroSat, we observe that many methods obtain similar or better in-domain OOD detection performance compared to out-of-domain OOD detection performance. These results confirm the theoretical findings regarding domain feature collapse from Theorem \ref{mainbodygenloss}. 

It is also important to note that there is high variance for individual out-of-domain datasets when benchmarking. For example, when models are trained on the Colon ID dataset, they tend to struggle with detecting MNIST patterns as OOD. We observe that certain OOD sets are more problematic depending on the ID set. This suggests that only some OOD datasets contain features that could be similar to the ID class features, supporting the idea that domain feature collapse occurs from an over-reliance on class features and the inability to identify domain features. 

In every case, adding a domain filter reduces the FPR@95 for out of domain OOD detection by a significant margin, sometimes trivializing the threat of out of domain OOD samples. For example, ReAct performs best in-domain on the Colon dataset, but suffers from extremely high FPR@95 of $61$\% on out of domain OOD samples. Adding a domain filter reduces this FPR rate to $0.7$\%, effectively eliminating the problem of out-of-domain OOD detection. In most cases, adding the domain filter reduces in domain performance by a marginal amount. 

These results demonstrate that domain feature collapse is a real problem across a wide variety of datasets. We also observe that domain filtering is a generally applicable  solution to address domain feature collapse. % but there are some trade offs in terms of... 

\begin{table*}
\centering
\caption{Summary OOD Performance Across All Datasets Reported As (In-Domain OOD Score)/(Out-of-Domain OOD Score). We exclude the Rock dataset from this summary as it is an outlier for reasons explained in Section \ref{discussion}.  Best scores are in bold (excluding PT KNN baseline). The domain filter methods are italicized. SC Resnet is not compatible with OOD methods that use logits.  See Appendix \ref{oodref} for OOD detection method descriptions and Appendix \ref{expresult} for more detailed results.}
\label{tab:summary}
\begin{tabular}{lllllll}
\toprule
 & \multicolumn{3}{l}{FPR@95 (Lower is Better)} & \multicolumn{3}{l}{AUROC (Higher is Better)} \\
 & CE DinoV2 & CE Resnet & SC Resnet & CE DinoV2 & CE Resnet & SC Resnet \\
Method &  &  &  &  &  &  \\
\midrule
PT KNN & 79.7 / 0.9 & 79.7 / 0.9  & 79.7 / 0.9  & 65.1 / 99.6 & 65.1 / 99.6 & 65.1 / 99.6 \\
MSP & 65.4 / 43.0 & 61.8 / 38.9 & NA & 75.1 / 82.0 & 78.3 / 87.4 & NA \\
Energy & 65.0 / 37.3 & 65.3 / 41.4 & NA & 75.3 / 85.6 & 78.0 / 87.6 & NA \\
Mahalanobis & 62.5 / 18.5 & 59.9 / 16.2 & 62.3 / 34.7 & 75.9 / 93.4 & 78.4 / 94.4 & \textbf{78.9} / 87.6 \\
NCI & 66.7 / 35.3 & 74.5 / 36.1 & NA & 74.1 / 86.6 & 73.3 / 88.5 & NA \\
KNN & \textbf{61.9} / 25.4 & 64.4 / 25.8 & \textbf{61.5} / 32.9 & 75.8 / 91.0 & 76.1 / 91.1 & 78.0 / 87.8 \\
ReAct & 64.2 / 36.4 & 71.9 / 47.7 & NA & 75.9 / 86.3 & 74.4 / 84.9 & NA \\
\textit{DF + KNN} & 65.2 / 3.2 & 64.3 / \textbf{2.5} & 63.8 / \textbf{3.2} & 73.9 / 99.0 & 75.8 / \textbf{99.2} & 76.2 / \textbf{99.0} \\
\textit{DF + ReAct} & 64.3 / 3.7 & 72.3 / 4.1 & NA & 75.7 / 98.8 & 73.8 / 99.1 & NA \\
\textit{DF + MDS} & 62.1 / \textbf{2.9} & \textbf{60.0} / 11.8 & 62.4 / 11.6 & \textbf{76.1} / \textbf{99.0} & \textbf{78.6} / 96.3 & 78.1 / 95.2 \\
\bottomrule
\end{tabular}
\end{table*}



\begin{table*}
\centering
\caption{Summary FPR@95 OOD Performance Across All Datasets for Selected ID Datasets Reported As (In-Domain OOD Score)/(Out-of-Domain OOD Score). Best scores are in bold (excluding PT KNN baseline). See Appendix \ref{oodref} for OOD detection method descriptions and Appendix \ref{expresult} for more detailed results.}
\label{tab:fpr95}
\begin{tabular}{lllllll}
\toprule
& Colon & Eurosat & Food & Garbage & Rock & Tissue \\
Method &  &  &  &  &  &  \\
\midrule
PT KNN & 67.4 / 0.0 & 69.1 / 0.3 & 80.0 / 0.6 & 87.2 / 0.4 & 91.9 / 6.6 & 89.3 / 0.0 \\
MSP & 59.1 / 53.0 & \textbf{41.3} / 49.8 & 74.9 / 63.7 & 68.0 / 42.0 & 85.8 / 71.8 & 84.2 / 76.6 \\
Energy & 61.0 / 70.7 & 42.5 / 50.1 & 75.2 / 62.9 & 78.7 / 54.3 & 86.7 / 71.2 & 84.4 / 79.2 \\
Mahalanobis & 40.8 / 12.5 & 51.4 / 13.7 & 76.8 / 52.1 & \textbf{59.8} / 13.9 & 83.1 / 44.2 & 91.4 / 3.8 \\
NCI & 74.5 / 24.8 & 72.7 / 57.1 & 80.4 / 65.4 & 74.2 / 31.4 & 75.9 / 64.0 & 84.5 / 35.7 \\
KNN & 40.0 / 13.2 & 48.4 / 31.3 & \textbf{73.3} / 62.7 & 77.9 / 33.3 & 77.3 / 61.8 & 92.6 / 31.6 \\
ReAct & \textbf{39.0} / 61.2 & 55.5 / 54.4 & 85.9 / 71.1 & 82.9 / 58.6 & 84.7 / 75.0 & \textbf{81.7} / 48.0 \\
\textit{DF + KNN} & 41.5 / \textbf{0.2} & 49.6 / \textbf{1.5} & 73.5 / \textbf{2.3} & 76.5 / 2.1 & 75.1 / 52.5 & 92.2 / 0.4 \\
\textit{DF + ReAct} & 40.6 / 0.7 & 65.2 / 4.3 & 86.4 / 2.2 & 82.9 / \textbf{1.8} & 84.9 / 61.0 & 81.9 / \textbf{0.7} \\
\textit{DF + MDS} & 40.4 / 6.9 & 51.4 / 10.4 & 76.6 / 39.1 & 61.6 / 12.7 & \textbf{82.0} / \textbf{39.8} & 91.3 / 0.9 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}
\centering
\caption{Detailed FPR@95 OOD Detection Performance for the Colon Dataset. See Appendix \ref{oodref} for OOD detection method descriptions. Best scores are in bold (excluding PT KNN baseline).}
\label{tab:fprcolon}
\begin{tabular}{llllllllll}
\toprule
OOD Dataset & In Domain  & Chest & Cifar10 & Cifar100 & Mnist & Place365 & Svhn & Texture & Tin \\
Method &  (Adjacent) &  &  &  &  &  &  &  &  \\
\midrule
PT KNN & 67.4 & 0.1 & 0.0 & 0.0 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 \\
MSP & 59.1 & 3.3 & 46.8 & 66.9 & 38.2 & 36.9 & 63.5 & 96.1 & 72.4 \\
Energy & 61.0 & 3.1 & 89.1 & 92.1 & 41.1 & 75.5 & 77.4 & 98.5 & 89.3 \\
Mahalanobis & 40.8 & 16.7 & 8.7 & 9.0 & 27.7 & 6.1 & 15.6 & 13.2 & 3.1 \\
NCI & 74.5 & 11.0 & 24.2 & 24.5 & 14.4 & 29.2 & 42.4 & 28.5 & 24.4 \\
KNN & 40.0 & 12.4 & 11.9 & 11.6 & 26.7 & 4.3 & 16.0 & 17.8 & 5.0 \\
ReAct & \textbf{39.0} & 41.0 & 62.6 & 64.2 & 45.7 & 52.4 & 77.2 & 74.2 & 72.3 \\
DF + KNN & 41.5 & \textbf{0.2} & \textbf{0.2} & \textbf{0.2} & \textbf{0.3} & \textbf{0.1} & \textbf{0.2} & \textbf{0.3} & \textbf{0.1} \\
DF + ReAct & 40.6 & 0.4 & 0.8 & 0.8 & 0.5 & 0.6 & 0.9 & 0.9 & 0.8 \\
DF + MDS & 40.4 & 8.9 & 4.6 & 4.7 & 15.5 & 3.3 & 8.6 & 7.3 & 1.8 \\
\bottomrule
\end{tabular}
\end{table*}

