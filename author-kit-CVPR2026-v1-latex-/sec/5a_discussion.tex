\section{Discussion: On the Limitations of Domain Filtering}
\label{discussion}

\paragraph{Wide Domains.} On some datasets, DinoV2 Domain Filtering has difficulty with outliers, resulting in a very large distance threshold $t_\rvd$ and poor domain filter performance. The Rock dataset \citep{rock_data} would often set $t_\rvd \approx 1.78$, compared with the Colon dataset at $t_\rvd \approx 0.47$ and the Food dataset at $t_\rvd \approx 1.08$. By changing $p=0.99\to0.98$, we can reduce $FPR@95=52.5\to27.9 $ for the Rock dataset on out-of-domain OOD detection. One example of an outlier in the Rock dataset is an image of a marble countertop, as shown in Figure \ref{fig:rock} in the Appendix. See Appendix \ref{percentile} for a more detailed analysis of percentiles. % See appendix \ref{expresult}

\paragraph{Performance Cap.} One major problem with domain filtering is the strict nature of its false positive rate. For in-domain data that is of a similar distribution to the training data, we expect a minimum false positive rate equal to $FPR = 1 - p$. We find that increasing $p$ works well if the domain is narrow, but can significantly harm out-of-domain performance if there are outliers; see Appendix \ref{percentile}.

\paragraph{Unseen Domains.} Readers may question the viability of domain filtering when both the ID set and OOD set are unknown to the domain filtering model. In other words, since a pretrained DinoV2 model has seen such a wide variety of images, it may have already seen images similar to those in the out of domain OOD set. To address this concern, we included the chest Xray dataset \citep{yang2023medmnist} to show that a pretrained DinoV2 can filter between two unseen medical domains quite well (achieving 0.1 FPR@95 with the colon dataset as ID and Chest Xrays as OOD).

\paragraph{Mutual Information Measurement.} It is important to clarify that our experiments do not directly measure or prove that $I(\rvx_\rvd; \rvz) = 0$ for supervised representations. Instead, they establish a lower bound on $I(\rvx_\rvd; \rvz)$ through OOD detection performance. By Fano's inequality, poor separation performance necessarily implies a low lower bound on mutual information, while high separation performance implies a high lower bound. We acknowledge that cases may exist where $I(\rvx_\rvd; \rvz)$ is large yet OOD detection performance remains poor, potentially due to inefficient utilization of domain information for detection. However, our results clearly establish that the lower bound on $I(\rvx_\rvd; \rvz)$ is substantially higher when using the two-stage detector with pretrained representations compared to supervised representations alone. This quantifies the information-theoretic gap predicted by our theory and demonstrates the practical benefit of preserving domain information in the representation space.

