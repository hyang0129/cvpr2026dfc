\section{Discussion: On the Limitations of Domain Filtering}
\label{discussion}

\paragraph{Wide Domains.} On some datasets, DinoV2 Domain Filtering has difficulty with outliers, resulting in a very large distance threshold $t_\rvd$ and poor domain filter performance. The Rock dataset \citep{rock_data} would often set $t_\rvd \approx 1.78$, compared with the Colon dataset at $t_\rvd \approx 0.47$ and the Food dataset at $t_\rvd \approx 1.08$. By changing $p=0.99\to0.98$, we can reduce $FPR@95=52.5\to27.9 $ for the Rock dataset on out-of-domain OOD detection. One example of an outlier in the Rock dataset is an image of a marble countertop, as shown in Figure \ref{fig:rock} in the Appendix. See Appendix \ref{percentile} for a more detailed analysis of percentiles. % See appendix \ref{expresult}

\paragraph{Performance Cap.} One major problem with domain filtering is the strict nature of its false positive rate. For in-domain data that is of a similar distribution to the training data, we expect a minimum false positive rate equal to $FPR = 1 - p$. We find that increasing $p$ works well if the domain is narrow, but can significantly harm out-of-domain performance if there are outliers; see Appendix \ref{percentile}.

\paragraph{Unseen Domains.} Readers may question the viability of domain filtering when both the ID set and OOD set are unknown to the domain filtering model. In other words, since a pretrained DinoV2 model has seen such a wide variety of images, it may have already seen images similar to those in the out of domain OOD set. To address this concern, we included the chest Xray dataset \citep{yang2023medmnist} to show that a pretrained DinoV2 can filter between two unseen medical domains quite well (achieving 0.1 FPR@95 with the colon dataset as ID and Chest Xrays as OOD). 

